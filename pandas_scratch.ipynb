{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyData Cardiff Workshop 3 - Introduction to Pandas\n",
    "\n",
    "![title](images/pydata_cardiff.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to the library\n",
    "\n",
    "Pandas is a seminal python library, which has revolutionised data analytics for the programming language. It began development in 2008 by Wes McKinney when he was working at AQR Capital Management. Initially, it was a purely in-house project, but on leaving his position, Wes was able to convince AQR to permit him to open-source the code.\n",
    "\n",
    "If anyone is interested - the name Pandas stands for PANel Data ANalysis\n",
    "\n",
    "Note that the usual way to import this library is to use the pattern `import pandas as pd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing the basic data types\n",
    "\n",
    "### The Pandas Series\n",
    "\n",
    "A one dimensional array of information. It has similarities with a numpy array - and it can be useful to think of a series like a column of information in an Excel Spreadsheet. Similarly to the numpy array - all of the data types in a series _should_ be of the same data type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a simple Series - very similar to a `numpy array`\n",
    "\n",
    "Note the present of a single integer at the end - but this will be coerced to a float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar = np.array([0.2, 1.2, 3.4, 5.6, 3.8, 6.7, 1.2, 7])\n",
    "ser = pd.Series([0.2, 1.2, 3.4, 5.6, 3.8, 6.7, 1.2, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note how the series deals with Mixed types\n",
    "\n",
    "It states that they are of type `'O'` - meaning a Python object!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_ser = pd.Series([1, 'hello', None, 3.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_ser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_ser.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similar methods and functionality\n",
    "\n",
    "There are a series of methods of the Series that share the same functionality with numpy arrays. There are called the numpy 'universal' functions `ufunc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### However!\n",
    "\n",
    "There will be some different behaviours seen! Note the different ways in which the variance is calculated.\n",
    "\n",
    "In numpy - this is calculated as:\n",
    "\n",
    "$$\\frac{\\Sigma (x - \\bar{x})^{2}}{n}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But in the Series - this is calculated as the _unbiased_ variance, using a method called _Bessel's Correction_ by subtracting 1 from _n_\n",
    "\n",
    "$$\\frac{\\Sigma (x - \\bar{x})^{2}}{n - 1}$$\n",
    "\n",
    "The effect that this has is a larger value for variance. In statistics - this has useful implications by making the variance of distributions wider, and statistical testing more rigorous.\n",
    "\n",
    "This value can be set by chaning the _delta degrees of freedom_ argument `ddof`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser.var(ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser.var(ddof=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can always call the series with `.values` to get the information as a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(ser.values, np.ndarray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Series Index\n",
    "\n",
    "This is a key feature of the Series when compared with the array - and can be thought of as the name that the a row would be given if the Series was a column in a SpreadSheet.\n",
    "\n",
    "This can be seen when we simply view the object - note that as we did not set this, the default value is the number of the row - indexed from 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be set at the creation of the variable - and note that we can use the values of the previous series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser2 = pd.Series(data=ser.values, index = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this way - the series can be interacted with in a similar fashion to a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "di = {'a': 0.2, 'g': 1.2, 'h': 7., 'b': 1.2, 'c': 3.4, 'f': 6.7, 'd': 5.6, 'e': 3.8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser2['c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "di['c']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However - note that there is an additional slicing ability that is not present in dictionaries\n",
    "\n",
    "__BUT__ - take care to notice that this slicing in Pandas is __inclusive__ of the end point!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser2['c': 'f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "di['c': 'f']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional functionality in the Series\n",
    "\n",
    "A good example of this are the functions `rolling` and `expanding`. These create a type of _Window_ function - either sliding or expanding.\n",
    "\n",
    "Note the presence of the missing values when calling these functions. In this case, the first two values are first used to calculate the third value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser2.expanding(3).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser2.rolling(3).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also functionality to shift the data by position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser2.shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser2.shift(-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values - differences between numpy and pandas\n",
    "\n",
    "One feature of numpy arrays is that the presence of missing values can have a detrimental effect when performing any `func`\n",
    "\n",
    "Note that we __must__ use the `np.nan` (not a number) variable to create the missing value - `None` will not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_missing = np.array([1, 2, 3, 4, np.nan, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_missing.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_missing.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slight difference here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_missing.cumsum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has to be dealt with using the specialised functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nansum(ar_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanmean(ar_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nancumsum(ar_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Pandas - these function __are the default!__\n",
    "\n",
    "Also - note that we can create a missing value using `None` - it will get changed to a `NaN` automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser_missing = pd.Series([1, 2, 3, 4, None, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser_missing.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser_missing.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one is slightly different!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser_missing.cumsum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with missing values\n",
    "\n",
    "Missing values are a common feature of using real datasets. 4 examples of how to deal with these are shown here.\n",
    "\n",
    "1. Replacing the missing value with a stated replacement\n",
    "2. Backfilling the data from later/lower\n",
    "3. Forward filling the data from earlier/higher\n",
    "4. Just drop them entirely!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser_missing_start = ser2.shift(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser_missing_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser_missing_start.fillna(-999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that missing numbers won't affect the mean calculation in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser_missing_start.fillna(ser_missing_start.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser_missing_start.bfill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser_missing_end = ser2.shift(-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser_missing_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser_missing_end.ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser_missing_end.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving to the DataFrame\n",
    "\n",
    "This is really the main datatype in Pandas. Think of one as a collection of Series objects - all sharing the same index.\n",
    "\n",
    "A dataframe can be created using a variety of methods - only a few of which will be shown here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a dictionary. However in order to maintain the desired column order - we will be using an `OrderedDict` here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = OrderedDict({\n",
    "    'col1': [1, 2, 3, 4],\n",
    "    'col2': [4, 5, 6, 7]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a numpy array, with column information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = np.array([\n",
    "    [1, 4],\n",
    "    [2, 5],\n",
    "    [3, 6],\n",
    "    [4, 7]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(data2, columns=['col1', 'col2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar fashion to a series - we can use `.values` to get the data as a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index can also be set at creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame(data1, index=['a', 'b', 'c', 'd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the columns and index __must__ be of the correct length, or you will get an error!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# err = pd.DataFrame(data1, index=['a', 'b', 'c'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding and selecting data\n",
    "\n",
    "If we wish to add a column of information to the dataframe, we can use dictionary-like `[]`, just as long as the length of the value being assigned is of the correct length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['col3'] = [4, 3, 2, 1]\n",
    "df3['col4'] = [101, 102, 103, 104]\n",
    "df3['col5'] = [-1, -2, -3, -4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the `[]` notation to obtain a single series back from the dataframe, using the column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col2_series = df3['col2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col2_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(col2_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using double brackets - `[[]]`\n",
    "\n",
    "A very important feature to learn is that, while the `[]` notation returned a series, if we use double square brackets, then we do not get a series... but a __dataframe__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col2_df = df3[['col2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(col2_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As dataframes do not need to be 1D - we can use this method to select multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[['col1', 'col3']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using conditional statements to select information\n",
    "\n",
    "* This can include either single - or multiple statements\n",
    "* But note the syntax for how multiple statements are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[df3['col1'] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[(df3['col1'] > 1) & (df3['col2'] >= 6)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the `.iloc` and `.loc` notation\n",
    "\n",
    "This is often the preferred method of selecting data. It can seem a little strange - but this will hopefully break it down\n",
    "\n",
    "* We use `loc` for using identifiers present in the index\n",
    "* We use `iloc` when getting the numbers of the rows - indexed from 0\n",
    "    * Of course - if the index is the default of row numbers - then this will be the same!\n",
    "* The earlier feature of `[]` for series and `[[]]` still holds!\n",
    "\n",
    "Of note - you will sometime see the func `ix` used in some older text - this has now been deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.loc['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.loc[['a']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.iloc[[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting column  as well!\n",
    "\n",
    "Note that this will return the value that appears in a particular cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.loc['a', 'col3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slicing\n",
    "\n",
    "Using this method - we can use slicing for both rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.loc['a': 'd', 'col2': 'col4']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The `SettingWithCopy` warning!\n",
    "\n",
    "This will soon become the bane of your life when working with Pandas dataframes!\n",
    "\n",
    "Here I will try to explain it as best as I can!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df3.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4['a': 'c']['col2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4['a': 'c']['col2'] = [32, 31, 30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has still worked though!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df3.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `loc`, we avoid this error!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.loc['a': 'c', 'col2'] = [32, 31, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now - this seems to work here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df4.loc['a': 'c', ['col1', 'col3', 'col5']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.loc['a', 'col3'] = 9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = df4.loc['b': 'd', :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6.loc['a', 'col3'] = 9999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just observe how irritating this is!\n",
    "\n",
    "This really looks the same to me!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warning_data = {'one': np.arange(1, 11), 'two': np.arange(11, 21)}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warning_df = pd.DataFrame(warning_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warning_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warning_df2 = warning_df.loc[3:5, :] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warning_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warning_df2.loc[4, 'one'] = 99 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warning_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just make a copy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_warning_df = pd.DataFrame(warning_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_warning_df2 = no_warning_df.loc[3:5, :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_warning_df2.loc[4, 'one'] = 99 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_warning_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_warning_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is admittedly confusing! For a more detailed explanation - see [this blog](https://www.dataquest.io/blog/settingwithcopywarning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading in data\n",
    "\n",
    "This is probably the most important part of the workshop, as it will be one of the most common processes that you will __always__ do when carrying out data analysis. For this, we will look at loading in data from both a comma-separated-value file `.csv` and Excel files (other methods can include reading in streaming data - or information from relational databases). The format that you will probably be working with most is `.csv`. This is done using the following methods:\n",
    "\n",
    "* `pd.read_csv()`\n",
    "* `pd.read_excel()`\n",
    "    * Note that to use this - you must install the `xlrd` library to read\n",
    "    * And the `openpyxl` (together with its dependencies) to write data (use `pip` or `conda`)\n",
    "        * But we won't be using that here!\n",
    "\n",
    "This quickly can get more complicated that it initially sounds - a quick look at the documentation for these functions shows that! This is because of all of the potential problems that have to be considered when _parsing_ data from an external source. We do not have time to cover all of these, but a few of the features will be explained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data from a _clean_ `.csv` file\n",
    "\n",
    "* Note that this dataset does not have any index information - so one will be made with the row numbers indexed from 0\n",
    "* Also - the file does not _have_ to separated by commas - if any other punctuation is use (like `;`), then this can be specified with the `delimiter` or `sep` argument (they do exactly the same thing - violation of the Zen of Python!)\n",
    "    * If you know that your columns are segregated by spaces - or any other form of whitespace - then use the `delim_whitespace = True` in the function call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 5 data/iris.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_csv = pd.read_csv('data/iris.csv')\n",
    "\n",
    "# This is just the same as:\n",
    "# iris_csv = pd.read_csv('data', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the data with `.head()` and `.tail()`\n",
    "\n",
    "Probably the most used function that you will ever learn in Pandas is `head()`, which allows us to see the first 5 rows of data by default - but this number can be changed.\n",
    "\n",
    "`tail()` has similar functionality - but shows the end of the dataframe rather than the top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_csv.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_csv.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in from Excel\n",
    "\n",
    "Here - the syntax is very similar, but note that as the file in question has multiple sheets - we can specify the sheet name of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds = pd.read_excel('data/iris_and_diamonds.xlsx', sheet_name='diamonds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in some _problematic_ data!\n",
    "\n",
    "This file has 2 lines of junk information at the top of it - you will sometimes get it when downloading from certain sites - as they like to put it in there for identification purposes - and to make our work more interesting/unbelievably-irritating!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 5 data/iris_problem.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_csv2 = pd.read_csv('data/iris_problem.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_csv2 = pd.read_csv('data/iris_problem.csv', skiprows=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing out the data\n",
    "\n",
    "If we want to save our data into a text file - we can use the `to_csv()` function. The main thing to be careful of here is to make sure that you specify whether you want the index to be saved or not. by default - it will be, so if you don't have any meaningful information there, be sure to set it to False!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iris_csv2.to_csv('iris_with_index.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iris_csv2.to_csv('iris_no_index.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reality though - we could probably just delete this in the text file before we load it in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful information functions\n",
    "\n",
    "We will now look at 2 functions that can be used to get some summary information on the dataframes - we will stay with the iris dataset that we have loaded in already"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `info`\n",
    "\n",
    "This will give us basic information about:\n",
    "\n",
    "* The number of entries\n",
    "* The number of missing values\n",
    "* The data type of each column/series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_csv.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `describe`\n",
    "\n",
    "This gives us a basic statistical summary of the data\n",
    "\n",
    "* Note that by default - it will only include the numerical information\n",
    "* If we want information on any `object` or 'categorical' columns - we need to include `include='all'` in the arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_csv.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_csv.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregating information\n",
    "\n",
    "This is where the power of Pandas really comes into play. We can start performing database style operations on our data. Before Pandas - this was a particular headache!\n",
    "\n",
    "Here, we will be using the values in the `species` column to get aggregated values of the information in the numerical columns. This is referred to as a __Group By__ operation.\n",
    "\n",
    "Note that just calling a `groupby` returns a new type of object - this is because we have only _primed_ pandas to anticipate that we wish to do something with the grouping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_csv.groupby('species')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first example - we will take the average value of all of the numerical column by species. Note that the information that was in the species column __is now the index of the new dataframe!!!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_mean = iris_csv.groupby('species').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In built functions\n",
    "\n",
    "Notice that we only had to call the `mean` function on the grouped data, we can also use a variety of different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_sum = iris_csv.groupby('species').sum()\n",
    "iris_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_std = iris_csv.groupby('species').std()\n",
    "iris_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the generic `agg`\n",
    "\n",
    "These are all suitable to do something that is built in, but if we wish to do something more complicated - then we have to use the generic `agg` method (aggregation)\n",
    "\n",
    "In this first instance - we are performing multiple aggregations, and we now see the first example of a multi level column. We will be looking at how to deal with these soon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_mean_sum = iris_csv.groupby('species').agg(['mean', 'sum'])\n",
    "iris_mean_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using different aggregations for different columns\n",
    "\n",
    "Here - we are interested in having different types of aggregations for different columns. We will look at calculating:\n",
    "\n",
    "* The mean of the `sepal_length` per species\n",
    "* The sum of the `sepal_width` per species\n",
    "\n",
    "In order to do this, we pass in dictionaries. However, in the first case, note that we are losing the vital information about the levels of aggretation that are being performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_csv.groupby('species').agg({'sepal_length': 'mean', 'sepal_width': 'sum'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is a quirk of using Pandas\n",
    "\n",
    "In order to get around this - we need to enter in the aggregations that we want using lists. This way, we end up with having the multi level columns again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_csv.groupby('species').agg({'sepal_length': ['mean'], 'sepal_width': ['sum']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting rid of the multi levels\n",
    "\n",
    "Here is a neat trick that you can use if you do not wish to have the multi levels. We will also add in some standard deviation info as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_agg_df = iris_csv.groupby('species').agg({'sepal_length': ['mean'], 'sepal_width': ['sum', 'std']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_agg_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_agg_df.columns = ['_'.join(x) for x in multi_agg_df.columns.ravel()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom aggregations\n",
    "\n",
    "We do not have to rely on using built in aggregations on our grouped data. We can also use _anonymous_ functions, which in Python are called using `lambda`\n",
    "\n",
    "Here we calculate the mean of values greater than 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_csv.groupby('species').agg(lambda x: x[x > 1].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### But the column names!\n",
    "\n",
    "Another Pandas quirk here - if we want to add in the information about our custom aggregation - we need to pass in a __list of tuples!!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_csv.groupby('species').agg([('mean > 1', lambda x: x[x > 1].mean())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivot Tables\n",
    "\n",
    "Here is a quick example of a common piece of functionality that is often used in Excel - the Pivot Table.\n",
    "\n",
    "This is another way of aggregating data - but note now that the dataframe is transposed from what was there before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_csv.pivot_table(columns='species', aggfunc='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use lists to keep the information in multi level columns - and perform multiple types of aggregation as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_csv.pivot_table(columns='species', aggfunc=['mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_csv.pivot_table(columns='species', aggfunc=['mean', 'sum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_pivot = iris_csv.pivot_table(columns='species', aggfunc=['mean', 'sum'])\n",
    "iris_pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another column hack!\n",
    "\n",
    "Notice the reordering of the levels - the `[::-1]` slice means 'traverse from the end to the start'\n",
    "\n",
    "Don't worry if you are not familiar with this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_pivot.columns = ['_'.join(x[::-1]) for x in iris_pivot.columns.ravel()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = [1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li[::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li[::1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Level GroupBys\n",
    "\n",
    "Here, we will show that we can use the `groupby` on multiple levels. The iris dataset is not suitable for this as it only has the species column to perform any meaningfull operations on. So we will use the diamonds data that we loaded in from Excel earlier.\n",
    "\n",
    "Here, I was to get the mean value of all the numerical columns based on the 2 columns:\n",
    "\n",
    "* `cut`\n",
    "* `clarity`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we now get a `MultiIndex`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_mean = diamonds.groupby(['cut', 'clarity']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting information from `MultiIndex`\n",
    "\n",
    "This can be quite a difficult topic, and is a bit more advanced, but I wanted to show you here as a future reference.\n",
    "\n",
    "1. Drop the index and then use `loc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_mean2 = diamonds_mean.reset_index()\n",
    "diamonds_mean2.loc[diamonds_mean2['cut'] == 'Fair']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We can use `xs` to get a cross section. But note that this can only be used on single levels within the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_mean.xs('Fair', level='cut')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_mean.xs('IF', level='clarity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_mean.xs(('Good', 'VS2'), level=('cut', 'clarity'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. For multiple levels in multiple columns - we need to use an `IndexSlice` - __this is complicated!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.IndexSlice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_mean.loc[idx[['Good', 'Fair'], ['IF', 'SI1']], ['depth', 'price']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series data and Plotting from Pandas\n",
    "\n",
    "As the origins of Pandas are in the financial sector, there is a large amount of functionality for time based data. In this type of data, the index  really starts to play a key role, and becuase a `DatetimeIndex`.\n",
    "\n",
    "In order to illustrate this, we will be loading in some financial data that was downloaded from Yahoo Finance. It is the Open/High/Low/Close data from Tesla for the past year.\n",
    "\n",
    "Note that we are now specifying the column to use as an index. However - this is not enough to ensure a `DatetimeIndex`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsla = pd.read_csv('data/TSLA.csv', index_col='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsla.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `parse_dates`\n",
    "\n",
    "The following command instructs Pandas to read in data as dates - if this can be done.\n",
    "\n",
    "In this situation it works, as the data is formatted so that it can be easily read in as a date. However, if this information was more bespoke, then you will have to set a value for the `date_parser` argument using additional information from the `datetime` built in Python library. However, this is outside of the scope of this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsla = pd.read_csv('data/TSLA.csv', index_col='Date', parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsla.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the information\n",
    "\n",
    "When working in a notebook - be sure that you have set `%matplotlib inline` before you do any plotting.\n",
    "\n",
    "After this is done - to plot a Pandas series, simply call the `plot` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsla['Adj Close'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default size is a little small, so we can add in the figsize argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsla['Adj Close'].plot(figsize=(16, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we plot a dataframe instead - we get legend information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsla[['Adj Close']].plot(figsize=(16, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting multiple lines\n",
    "\n",
    "This is as simple as passing in multiple column names. Note that Pandas required the data to be in 'wide' format to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsla[['Open', 'Close']].plot(figsize=(16, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple assignments and transform\n",
    "\n",
    "Here we want to get information about the starting price of the stock each month. In order to do this:\n",
    "\n",
    "* We first need to get the year and month information from the index\n",
    "* The we perform a `transform` operation based on the year/month groupings\n",
    "    * This is called a 'Window Function' - and also makes use of group by\n",
    "    * Here we are using a built in function called `first`\n",
    "    * We are also explicitly sorting by the index to ensure the ordering is correct\n",
    "    * This might not be necessary - but I like to be sure!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsla['year'] = tsla.index.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsla['month'] = tsla.index.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsla['ACMS'] = tsla.sort_index().groupby(['year', 'month'])['Adj Close'].transform('first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsla[['Adj Close', 'ACMS']].plot(figsize=(16, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple assignments\n",
    "\n",
    "It is possible to do all of the new columns in a single chain of functions using the `assign` operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsla = pd.read_csv('data/TSLA.csv', index_col='Date', parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsla_first = (\n",
    "    tsla\n",
    "    .assign(year = lambda df: df.index.year)\n",
    "    .assign(month = lambda df: df.index.month)\n",
    "    .assign(ACMS = lambda df: df.groupby(['year', 'month'])['Adj Close'].transform('first'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsla_first[['ACMS', 'Adj Close']].plot(figsize=(16, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to use strings for the column names instead, you can use the `**` syntax to 'unpack' a dictionary into keyword arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsla_first = (\n",
    "    tsla\n",
    "    .assign(**{'year': lambda df: df.index.year, 'month': lambda df: df.index.month})\n",
    "    .assign(**{'ACMS': lambda df: df.groupby(['year', 'month'])['Adj Close'].transform('first')})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsla_first[['ACMS', 'Adj Close']].plot(figsize=(16, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do some joins and melt/pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
